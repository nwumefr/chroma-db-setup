{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e45ecbb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup env\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17a67d22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# our prompt\n",
    "prompt = \"what is a pod in kubernetes? is it just a docker container?\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88999c03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup chroma client\n",
    "import chromadb\n",
    "chroma_client = chromadb.PersistentClient('./tmp/chroma-db')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51ed1a5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# custom embedding function using Gemini Embeddings API\n",
    "# ref: https://github.com/google-gemini/cookbook/blob/main/examples/chromadb/Vectordb_with_chroma.ipynb\n",
    "import google.genai as genai\n",
    "\n",
    "client = genai.Client(api_key=os.getenv(\"GENAI_API_KEY\"))\n",
    "\n",
    "class GeminiEmbeddingFunction(chromadb.EmbeddingFunction):\n",
    "  def __call__(self, input: chromadb.Documents) -> chromadb.Embeddings:\n",
    "    EMBEDDING_MODEL_ID = \"gemini-embedding-001\"  # @param [\"gemini-embedding-001\", \"text-embedding-004\"] {\"allow-input\": true, \"isTemplate\": true}\n",
    "    title = \"Custom query\"\n",
    "    response = client.models.embed_content(\n",
    "        model=EMBEDDING_MODEL_ID,\n",
    "        contents=input,\n",
    "        config=genai.types.EmbedContentConfig(\n",
    "          task_type=\"retrieval_document\",\n",
    "          title=title\n",
    "        )\n",
    "    )\n",
    "\n",
    "    return response.embeddings[0].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7ab0c8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create collection for storage\n",
    "collection = chroma_client.get_collection(name=\"test\",\n",
    "                                             embedding_function=GeminiEmbeddingFunction()\n",
    "                                             )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5069d9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function for chunking text into smaller pieces for better embedding and retrieval\n",
    "# makes use of huggingface tokenizers library to tokenize text\n",
    "#  and chunk it based on a specified chunk size\n",
    "\n",
    "import transformers\n",
    "from transformers import AutoTokenizer, GPT2Tokenizer\n",
    "# define tokenizer\n",
    "tokenizer:GPT2Tokenizer = AutoTokenizer.from_pretrained('openai-community/gpt2-medium')\n",
    "# chunking function\n",
    "def chunker(text, chunk_size=5) -> list[str]:\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    # get length of tokens\n",
    "    _len = len(tokens)\n",
    "    chunks = []\n",
    "    chunk = []\n",
    "    for i in range(_len):\n",
    "        if i%chunk_size==0 and i!=0:\n",
    "            print(chunk)    \n",
    "            print('chunk limit reached')\n",
    "            chunks.append(tokenizer.convert_tokens_to_string(chunk))\n",
    "            chunk = []\n",
    "        chunk.append(tokens[i])\n",
    "        print(f'Token {i}: {tokens[i]}')\n",
    "        \n",
    "    if chunk:  # Append the last chunk if it exists\n",
    "        print(chunk)\n",
    "        chunks.append(tokenizer.convert_tokens_to_string(chunk))\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60eabc46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read text from test_docs dir\n",
    "# note we will not be doing file readong for the scraper most likely\n",
    "# as far as i (chi) understand\n",
    "# companies usually run scrapers through timed jobs or specific triggers\n",
    "# that manage the scraping and data ingestion process\n",
    "# a common approach is to have sth like an aws lambda function or a serverless function\n",
    "# that gets triggered when new data is available or at scheduled intervals \n",
    "# to perform the scraping and then directly ingest the data \n",
    "# into the vector db\n",
    "\n",
    "import os\n",
    "import uuid\n",
    "\n",
    "docs = {}\n",
    "\n",
    "test_docs_dir = './test_docs'\n",
    "for filename in os.listdir(test_docs_dir):\n",
    "    print(filename)\n",
    "    if filename.endswith(('.txt','.md')):\n",
    "        with open(os.path.join(test_docs_dir, filename), 'rb') as f:\n",
    "            # read doc\n",
    "            content = f.read().decode('utf-8', errors='ignore')\n",
    "            # chunk doc into 500 token chunks\n",
    "            chunks = chunker(content, chunk_size=500)\n",
    "            # add chunks to docs dict with unique id\n",
    "            for i in chunks:\n",
    "                doc_id = str(uuid.uuid4())\n",
    "                docs[doc_id] = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6763f8d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pprint\n",
    "print(list(docs.keys()))\n",
    "pprint.pprint(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59aa9ffb",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in docs:\n",
    "    collection.add(ids=[i], documents=[docs[i]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bceebe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# another search test\n",
    "from pprint import pprint\n",
    "res = collection.query(\n",
    "    query_texts=[prompt],\n",
    "    n_results=4\n",
    ")\n",
    "# pprint(res)\n",
    "# print(len(res['documents']))\n",
    "doc_chunks = [i for i in res['documents'][0]]\n",
    "pprint(doc_chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1ccf690",
   "metadata": {},
   "outputs": [],
   "source": [
    "RAG_prompt_template = '''\n",
    "CONTEXT:\n",
    "{retrieved_documents}\n",
    "\n",
    "QUESTION:\n",
    "{user_question}\n",
    "\n",
    "INSTRUCTIONS:\n",
    "Answer the QUESTION using only the information provided in the CONTEXT above.\n",
    "Keep your answer grounded in the facts of the CONTEXT.\n",
    "Use [chunk_id] notation immediately after each statement to cite sources.\n",
    "If the CONTEXT doesn't contain enough information to fully answer the QUESTION, state: \"I don't have enough information to answer this completely\" and explain what's missing.\n",
    "Match the language of the user's QUESTION in your response.\n",
    "\n",
    "Provide a clear, factual answer based solely on the CONTEXT provided.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "834595d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# little llm test for fun\n",
    "from google import genai\n",
    "from google.genai.types import HttpOptions\n",
    "\n",
    "# format prompt with context from retrieved doc chunks\n",
    "formatted_prompt = RAG_prompt_template.format(\n",
    "    retrieved_documents='\\n\\n'.join(doc_chunks),\n",
    "    user_question=prompt\n",
    ")\n",
    "\n",
    "client = genai.Client(http_options=HttpOptions(api_version=\"v1\"))\n",
    "response = client.models.generate_content(\n",
    "    model=\"gemini-2.5-flash\",\n",
    "    contents=formatted_prompt,\n",
    "    config=genai.types.GenerateContentConfig(\n",
    "        max_output_tokens=1024,\n",
    "        temperature=0.2,\n",
    "        top_p=0.8,\n",
    "        stop_sequences=[\"###\"]\n",
    "    )\n",
    ")\n",
    "print(response.text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
